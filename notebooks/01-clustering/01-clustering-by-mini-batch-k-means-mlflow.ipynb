{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering by Mini Batch K-Means\n",
        "\n",
        "Here, we apply Mini Batch K-Means in attempt to segment data described by Recency, Frequency and Monetary Value of this group of customers. See [](../00-data/01-analyse-customer-value-by-frequency-recency-monetary-value.ipynb) for how the data is prepared. \n",
        "\n",
        "\n",
        "References: \n",
        "- [K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
        "- [Mini Batch K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)\n",
        "\n",
        "Notebooks Sequence:\n",
        "- [/00-data/00-explore-and-prepare-data.ipynb](../00-data/00-explore-and-prepare-data.ipynb)\n",
        "- [/00-data/01-analyse-customer-value-by-frequency-recency-monetary-value.ipynb](../00-data/01-analyse-customer-value-by-frequency-recency-monetary-value.ipynb)\n",
        "- [This Notebook](../01-clustering/00-clustering-by-mini-batch-k-means.ipynb)\n",
        "- [/02-interpretation/00-interprete.ipynb](../02-interpretation/00-interprete.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer, normalize\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data\n",
        "## Load Data\n",
        "\n",
        "Cell below assumed that dataset is registered in AML Workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# azureml-core of version 1.0.72 or higher is required\n",
        "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "# Get information about worksapce\n",
        "workspace = Workspace.from_config()\n",
        "workspace\n",
        "\n",
        "# Get dataset registered in AML by name\n",
        "dataset = Dataset.get_by_name(workspace, name='online-retail-frm')\n",
        "dataset_transformed = Dataset.get_by_name(workspace, name='online-retail-frm-transformed')\n",
        "\n",
        "# Convert Dataset to Pandas DataFrame\n",
        "df_orig = dataset.to_pandas_dataframe()\n",
        "df_transformed_orig = dataset_transformed.to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a copy\n",
        "df = df_orig.copy()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a copy\n",
        "df_transformed = df_transformed_orig.copy()\n",
        "df_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.01, random_state=9)\n",
        "df_train.shape\n",
        "df_train.head()\n",
        "df_test.shape\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Within Cluster Sum of Squared Errors (WCSS) aka Inertia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_wcss(min_cluster, max_cluster, batch_size, data):\n",
        "    \"\"\" Calculate Within Cluster Sum of Squared Errors (*WCSS*), i.e. km.inertia_ when iterate through min_cluster to max_cluster\n",
        "    \"\"\"\n",
        "    wcss=[]\n",
        "    for i in range(min_cluster, max_cluster):\n",
        "        km = MiniBatchKMeans(n_clusters=i,\n",
        "                             random_state=9,\n",
        "                             batch_size=batch_size,\n",
        "                             max_iter=100).fit(data)\n",
        "        km.fit(data)\n",
        "        wcss.append(km.inertia_)\n",
        "    return wcss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set parameterss\n",
        "min_cluster = 1\n",
        "max_cluster = 11\n",
        "batch_size = int(df.shape[0]*0.1)\n",
        "\n",
        "# run calculate_wcss\n",
        "wcss = calculate_wcss(min_cluster, max_cluster, batch_size, df_transformed) # note that df_transformed is used here\n",
        "\n",
        "wcss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieve the opitmal `k` by pre-defined threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimal_k(wcss):\n",
        "    \"\"\" Get optimal k\n",
        "    \"\"\"\n",
        "    # Get gradient\n",
        "    wcss_grad = np.gradient(wcss)\n",
        "\n",
        "    # Normalise gradient to maximum value\n",
        "    wcss_grad_norm = normalize(wcss_grad.reshape(1, -1), norm='max')\n",
        "\n",
        "    # Get optimal_k by pre-defined threshold\n",
        "    optimal_k = np.argmin(wcss_grad_norm < -0.15) + 1\n",
        "    \n",
        "    return optimal_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = get_optimal_k(wcss)\n",
        "k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define `sklearn.pipeline`\n",
        "References:\n",
        "- [User Guide](https://scikit-learn.org/stable/modules/compose.html#pipeline)\n",
        "- [`sklearn.pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure PowerTransformer\n",
        "ptransformer = PowerTransformer(method=\"yeo-johnson\")\n",
        "ptransformer\n",
        "\n",
        "# Configure kmeans\n",
        "n_clusters = 4\n",
        "batch_size = int(df_train.shape[0]*0.1)\n",
        "\n",
        "km = MiniBatchKMeans(n_clusters=n_clusters,\n",
        "                     random_state=9,\n",
        "                     batch_size=batch_size,\n",
        "                     max_iter=100)\n",
        "km\n",
        "\n",
        "pipeline = Pipeline(steps=[('ptransformer', ptransformer), ('mini-batch-k-means', km)],\n",
        "                    verbose=True)\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a new MLFlow experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Create an experiment\n",
        "experiment_id = mlflow.create_experiment(name='online-retail-customer-segmentation-mlflow', \n",
        "                                         tags={'purpose':'tutorial', 'pipeline':'sklearn.pipeline'})\n",
        "\n",
        "# Get experiment by experimnet_id\n",
        "experiment = mlflow.get_experiment(experiment_id=experiment_id)\n",
        "\n",
        "# Set this experiment as the active experiment\n",
        "experiment = mlflow.set_experiment(experiment_id=experiment_id)\n",
        "\n",
        "# Display\n",
        "experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imply input and output signature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Example input and output\n",
        "model_output = np.array([0, 2]) # example output, i.e. cluster label\n",
        "model_input = df.iloc[0:2]\n",
        "\n",
        "# Infer signature, i.e. input and output\n",
        "signature = infer_signature(model_input=model_input, model_output=model_output)\n",
        "signature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start autolog\n",
        "mlflow.sklearn.autolog() \n",
        "\n",
        "# Metrics to log\n",
        "metrics = {\"wcss\": wcss[k], \n",
        "           \"k\": k}\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    # fit pipeline\n",
        "    pipeline.fit(df) # note that df is used here\n",
        "\n",
        "    # log custom metrics\n",
        "    mlflow.log_metrics(metrics=metrics) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_id = run.info.run_id; run_id\n",
        "pipeline_model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\")\n",
        "type(pipeline_model)\n",
        "pipeline_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use model to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use trained model to predict using df_test\n",
        "pipeline_model.predict(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve `run` information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieve `run` information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retreive `artifacts`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve mlflow tracking\n",
        "client = mlflow.tracking.MlflowClient()\n",
        "client\n",
        "\n",
        "# List mlflow artifacts\n",
        "client.list_artifacts(run_id=run.info.run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload to Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if False:\n",
        "#if True:\n",
        "    from azureml.core import Workspace, Dataset\n",
        "\n",
        "    workspace = Workspace.from_config()\n",
        "    print(workspace.name, workspace.resource_group, workspace.location, workspace.subscription_id, sep = '\\n')\n",
        "\n",
        "    datastore = workspace.get_default_datastore()\n",
        "    datastore\n",
        "\n",
        "    # Save to local\n",
        "    filename = '../../.aml/data/online-retail-frm-train.csv'\n",
        "    df_train.to_csv(filename, index=False)\n",
        "\n",
        "    filename = '../../.aml/data/online-retail-frm-test.csv'\n",
        "    df_test.to_csv(filename, index=False)\n",
        "\n",
        "    # Upload to datastore\n",
        "    Dataset.File.upload_directory('../../.aml/data', datastore, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Dataframe as Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if False:\n",
        "#if True:\n",
        "    from azureml.core import Workspace, Dataset\n",
        "\n",
        "    workspace = Workspace.from_config()\n",
        "    workspace\n",
        "\n",
        "    datastore = workspace.get_default_datastore()\n",
        "    datastore\n",
        "\n",
        "    # Dataset name to register as \n",
        "    name = 'online-retail-frm-train'\n",
        "\n",
        "    # create a new dataset\n",
        "    Dataset.Tabular.register_pandas_dataframe(dataframe=df_train, \n",
        "                                              target=datastore, \n",
        "                                              name=name, \n",
        "                                              show_progress=True, \n",
        "                                              tags={'Purpose':'Tutorial'})\n",
        "\n",
        "    # Dataset name to register as \n",
        "    name = 'online-retail-frm-test'\n",
        "\n",
        "    # create a new dataset\n",
        "    Dataset.Tabular.register_pandas_dataframe(dataframe=df_test, \n",
        "                                              target=datastore, \n",
        "                                              name=name, \n",
        "                                              show_progress=True, \n",
        "                                              tags={'Purpose':'Tutorial'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('py38_segmentation_dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "82ae38ec8b3b4cdde76f4ac747612b1fbae548230d531a553aa6a98dd6660007"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
